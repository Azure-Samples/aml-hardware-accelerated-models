{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning VGG-SSD Object Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites for Local Training\n",
    "\n",
    "* CUDA 10.0, cuDNN 7.4\n",
    "* Recent Anaconda environment\n",
    "* Matplotlib\n",
    "* OpenCV-Python cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# install supported FPGA ML models, including VGG SSD\n",
    "# skip if already installed\n",
    "!pip install azureml-accel-models\n",
    "\n",
    "# Install Tensorflow. You may select to install Tensorflow for CPU or GPU.  \n",
    "# Instructions are here: https://pypi.org/project/azureml-accel-models/\n",
    "\n",
    "!pip install azureml-accel-models[gpu]\n",
    "#!pip install azureml-accel-models[cpu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys, glob\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tensorflow Finetuning Package\n",
    "sys.path.insert(0, os.path.abspath('../tfssd/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Training / Validation Data\n",
    "\n",
    "Images are .jpg files and annotations - .xml files in PASCAL VOC format.\n",
    "Each image file has a matching annotations file\n",
    "\n",
    "In this notebook we are looking for gaps on the shelves stocked with different products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "img = cv2.imread('sample.jpg')\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep import dataset_utils, pascalvoc_to_tfrecords\n",
    "from importlib import reload\n",
    "reload(dataset_utils)\n",
    "\n",
    "# Create directory for data files and model checkpoints.  \n",
    "\n",
    "from os.path import expanduser\n",
    "\n",
    "data_dir = expanduser(\"~/azml_ssd_vgg\")\n",
    "\n",
    "dataset_utils.create_dir(data_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify that annotations and images are in the correct folders\n",
    "\n",
    "data_dir_images = os.path.join(data_dir, \"JPEGImages\")\n",
    "data_dir_annotations = os.path.join(data_dir, \"Annotations\")\n",
    "classes = [\"stockout\"]\n",
    "\n",
    "if not os.listdir(data_dir_images) or not os.listdir(data_dir_annotations):\n",
    "    print('JPEGImages or Annotations folder is empty.  Please copy your images and annotations to these folders and rerun cell.')\n",
    "\n",
    "else:\n",
    "    images = glob.glob(os.path.join(data_dir_images, \"*.jpg\"))\n",
    "    annotations = glob.glob(os.path.join(data_dir_annotations, \"*.xml\"))\n",
    "    \n",
    "    # check for image and annotations files matching each other\n",
    "    \n",
    "    images, annotations = dataset_utils.check_labelmatch(images, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Into Training and Validation and Create TFRecord Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, test_images, \\\n",
    "    train_annotations, test_annotations = train_test_split(images, annotations, test_size = .2, random_state = 40)\n",
    "\n",
    "data_output_dir = os.path.join(data_dir, \"TFreccords\")\n",
    "\n",
    "pascalvoc_to_tfrecords.run(data_output_dir, classes, train_images, train_annotations, \"train\")\n",
    "pascalvoc_to_tfrecords.run(data_output_dir, classes, test_images, test_annotations, \"test\")\n",
    "\n",
    "print(os.listdir(data_output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and Run Training/Validation Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Data, Import the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune.train import TrainVggSsd\n",
    "from finetune.eval import EvalVggSsd\n",
    "\n",
    "ckpt_dir = data_dir\n",
    "# this is the directory where the original model to be\n",
    "# fine-tuned will be delivered and models saved as the training loop runs\n",
    "\n",
    "# get .tfrecord files created in the previous step\n",
    "train_files = glob.glob(os.path.join(data_output_dir, \"train_*.tfrecord\"))\n",
    "validation_files = glob.glob(os.path.join(data_output_dir, \"test_*.tfrecord\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for these epochs\n",
    "n_epochs = 6\n",
    "# steps per training epoch\n",
    "num_train_steps=3000\n",
    "# batch size. \n",
    "batch_size = 2\n",
    "# steps to save as a checkpoint\n",
    "steps_to_save=3000\n",
    "# using Adam optimizer. These are the configurable parameters\n",
    "learning_rate = 1e-4\n",
    "learning_rate_decay_steps=3000\n",
    "learning_rate_decay_value=0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_steps=156\n",
    "# number of classes. Includes the \"none\" (background) class\n",
    "# cannot be more than 21\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n_epochs):\n",
    "\n",
    "    with TrainVggSsd(ckpt_dir, train_files, \n",
    "                     num_steps=num_train_steps, \n",
    "                     steps_to_save=steps_to_save, \n",
    "                     batch_size = batch_size,\n",
    "                     learning_rate=learning_rate,\n",
    "                     learning_rate_decay_steps=learning_rate_decay_steps, \n",
    "                     learning_rate_decay_value=learning_rate_decay_value) as trainer:\n",
    "        trainer.train()\n",
    "\n",
    "    with EvalVggSsd(ckpt_dir, validation_files, \n",
    "                    num_steps=num_eval_steps, \n",
    "                    num_classes=num_classes) as evaluator:\n",
    "        evaluator.eval()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from finetune.inference import InferVggSsd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = 15, 15\n",
    "infer = InferVggSsd(ckpt_dir, gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classes, scores, boxes = infer.infer_file(test_images[5], visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "v-borisk"
   },
   {
    "name": "v-zaper"
   }
  ],
  "kernelspec": {
   "display_name": "python 3.6  brain",
   "language": "python",
   "name": "brain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
